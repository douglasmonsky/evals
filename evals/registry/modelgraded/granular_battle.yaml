granular_battle:
  prompt: |-
    You are comparing two provided responses to the same original task.
    
    [Original Task]
    {original_task_description}
    
    [Ideal Response]
    {ideal}
    
    [Response 1]
    {completion1}

    [Response 2]
    {completion2}

    Your task is the grade how well each response matches the ideal response. 
    
    [Scoring]
    4 points - Excellent: The LLM's output aligns nearly perfectly with the expected result. 
    It correctly captures all major and minor aspects, with only negligible differences, if any.
    The output could practically substitute the expected result.

    3 points - Good: The LLM's output captures the essence of the expected result, with minor differences or omissions.
    While not a perfect match, it maintains the overall meaning and purpose and provides a useful and accurate representation.
    
    2 points - Fair: The LLM's output only partially matches the expected result.
    While some aspects are correctly represented, there are notable inaccuracies, omissions,
    or irrelevancies that impact its overall usefulness and accuracy.
    
    1 point - Poor: The LLM's output has significant inaccuracies or is entirely irrelevant,
    offering little to no value in relation to the expected result.
    The output may miss the task's objective entirely or provide a solution that is inappropriate or incorrect.    
    
    [Final Output Formatting]
    Provide the answer in the following format:
    <Response 1 Score>x<Response 2 Score>
    Example: 1x2
  choice_strings:
    - "1x1"
    - "1x2"
    - "1x3"
    - "1x4"
    - "2x1"
    - "2x2"
    - "2x3"
    - "2x4"
    - "3x1"
    - "3x2"
    - "3x3"
    - "3x4"
    - "4x1"
    - "4x2"
    - "4x3"
    - "4x4"
  choice_scores:
    "1x1": 0.0
    "1x2": -1.0
    "1x3": -2.0
    "1x4": -3.0
    "2x1": 1.0
    "2x2": 0.0
    "2x3": -1.0
    "2x4": -2.0
    "3x1": 2.0
    "3x2": 1.0
    "3x3": 0.0
    "3x4": -1.0
    "4x1": 3.0
    "4x2": 2.0
    "4x3": 1.0
    "4x4": 0.0
  input_outputs:
    input1: completion1
    input2: completion2